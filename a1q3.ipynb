{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Christopher Zheng\n",
    "#260760794\n",
    "\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.stem import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from scipy.sparse import csc_matrix, dia_matrix, lil_matrix\n",
    "import re\n",
    "\n",
    "stopwords = set(sw.words(\"english\"))\n",
    "#print(stopwords)\n",
    "suggestive_words=[\"aren't\",\"didn't\",\"against\",\"couldn\",\"isn't\",\"wouldn\",\\\n",
    "                  \"doesn\",\"weren\",\"weren't\",\"ain\",\"should\",\"hasn\",\"mustn\",\"haven't\", \\\n",
    "                  \"hadn't\", \"doesn't\",'didn',\"shouldn't\",'isn','needn',\"won't\",'no',\\\n",
    "                  \"wasn't\", 'don',\"hasn't\",'hadn',\"mightn't\",'mightn','not',\"don't\", \\\n",
    "                  \"couldn't\",'haven',\"mustn't\",'nor','aren','wasn', \"wouldn't\", \"should've\"]\n",
    "stopwords = stopwords - set(suggestive_words)\n",
    "\n",
    "\n",
    "def generate_unigrams(s):\n",
    "    # Convert to lowercases. Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s.lower())\n",
    "    # Break sentence in the token, remove empty tokens and stopwords\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\" and token not in stopwords]\n",
    "\n",
    "    #lemmatize\n",
    "    tokens = [WordNetLemmatizer().lemmatize(token) for token in tokens]\n",
    "    \n",
    "#      # stem\n",
    "#     tokens = [PorterStemmer().stem(token) for token in tokens]\n",
    "    \n",
    "    # Concatentate the tokens into unigrams and return\n",
    "    unigrams = zip(*[tokens[i:] for i in range(1)])\n",
    "    return [\" \".join(unigram) for unigram in unigrams]\n",
    "\n",
    "#print(generate_unigrams(\"hello world hahah hahah hahahhhahaha I am Christopher\"))\n",
    "\n",
    "#loading the reviews as input\n",
    "pos_reviews = [line.rstrip('\\n') for line in open('input/rt-polarity.pos', encoding='latin-1')]\n",
    "neg_reviews = [line.rstrip('\\n') for line in open('input/rt-polarity.neg', encoding='latin-1')]\n",
    "\n",
    "pos_tokens = [generate_unigrams(line) for line in pos_reviews]\n",
    "neg_tokens = [generate_unigrams(line) for line in neg_reviews]\n",
    "dimension_y = len(pos_tokens) + len(neg_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freq_dict={} # token frequency\n",
    "for line in pos_tokens:\n",
    "    for token in line:\n",
    "        if token not in token_freq_dict:\n",
    "            token_freq_dict[token] = 1\n",
    "        else:\n",
    "            token_freq_dict[token] += 1\n",
    "            \n",
    "for line in neg_tokens:\n",
    "    for token in line:\n",
    "        if token not in token_freq_dict:\n",
    "            token_freq_dict[token] = 1\n",
    "        else:\n",
    "            token_freq_dict[token] += 1\n",
    "\n",
    "####VERY IMPORTANT THRESHOLD FOR INFREQUENT WORDS###\n",
    "infreq=[]\n",
    "for key in token_freq_dict:\n",
    "    if token_freq_dict[key] < 2:\n",
    "        infreq.append(key)\n",
    "\n",
    "for key in infreq:\n",
    "    del token_freq_dict[key]\n",
    "# #################################################\n",
    "        \n",
    "token_dict={}\n",
    "counter = 0 # count to offset\n",
    "pos_infreq=[]\n",
    "neg_infreq=[]\n",
    "\n",
    "for line in pos_tokens:\n",
    "    for token in line:\n",
    "        if token not in token_dict and token in token_freq_dict:\n",
    "            token_dict[token] = counter\n",
    "            counter = counter + 1\n",
    "        if token not in token_freq_dict:\n",
    "            pos_infreq.append(token)\n",
    "            \n",
    "\n",
    "for line in neg_tokens:\n",
    "    for token in line:\n",
    "        if token not in token_dict and token in token_freq_dict:\n",
    "            token_dict[token] = counter\n",
    "            counter = counter + 1\n",
    "        if token not in token_freq_dict:\n",
    "            neg_infreq.append(token)\n",
    "            \n",
    "##### Update tokens - remove infrequent tokens from the dict\n",
    "for word in infreq:\n",
    "    for line in pos_tokens:\n",
    "        for token in line:\n",
    "            if token == word:\n",
    "                line.remove(token)\n",
    "    for line in neg_tokens:\n",
    "        for token in line:\n",
    "            if token == word:\n",
    "                line.remove(token)\n",
    "\n",
    "for word in pos_infreq:\n",
    "    for line in pos_tokens:\n",
    "        for token in line:\n",
    "            if token == word:\n",
    "                line.remove(token)\n",
    "    for line in neg_tokens:\n",
    "        for token in line:\n",
    "            if token == word:\n",
    "                line.remove(token)\n",
    "                \n",
    "for word in neg_infreq:\n",
    "    for line in pos_tokens:\n",
    "        for token in line:\n",
    "            if token == word:\n",
    "                line.remove(token)\n",
    "    for line in neg_tokens:\n",
    "        for token in line:\n",
    "            if token == word:\n",
    "                line.remove(token)\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    return x / x.sum()\n",
    "\n",
    "def vectorize(tokens,groundtruth):\n",
    "    # one vector may look like (x,x,x,x,......,1 or 0)\n",
    "    vector = np.zeros(1 + len(token_dict))\n",
    "    \n",
    "    for token in tokens:\n",
    "        vector[token_dict[token]] += 1\n",
    "    \n",
    "    vector = normalize(vector)\n",
    "    vector[-1] = groundtruth\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_x = 1 + len(token_dict)\n",
    "final_matrix = np.zeros((dimension_y,dimension_x))\n",
    "# print(dimension_y)\n",
    "# print(dimension_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PROGRAMS\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:81: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "counter = 0 # again, count to offset\n",
    "# now fill in the vectors into the matrix\n",
    "# 1 for positive reviews\n",
    "# 0 for negative reviews\n",
    "for tokens in pos_tokens:\n",
    "    final_matrix[counter,:] = vectorize(tokens,1)\n",
    "    counter = counter + 1\n",
    "for tokens in neg_tokens:\n",
    "    final_matrix[counter,:] = vectorize(tokens,0)\n",
    "    counter = counter + 1\n",
    "#print(len(final_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(final_matrix)\n",
    "# preprocess to handle NaN -> Zero\n",
    "where_are_NaNs = np.isnan(final_matrix)\n",
    "final_matrix[where_are_NaNs] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training / test set separation\n",
    "np.random.shuffle(final_matrix)\n",
    "label_matrix = final_matrix[:,-1]\n",
    "feature_matrix = final_matrix[:,:-1]\n",
    "\n",
    "# training_feature = feature_matrix[:int(len(final_matrix)*0.7),]\n",
    "# training_groundtruth = label_matrix[:int(len(final_matrix)*0.7),]\n",
    "# test_feature = feature_matrix[int(len(final_matrix)*0.7):,]\n",
    "# test_groundtruth = label_matrix[int(len(final_matrix)*0.7):,]\n",
    "\n",
    "training_feature = feature_matrix[:int(len(final_matrix)*0.8),]\n",
    "training_groundtruth = label_matrix[:int(len(final_matrix)*0.8),]\n",
    "test_feature = feature_matrix[int(len(final_matrix)*0.8):,]\n",
    "test_groundtruth = label_matrix[int(len(final_matrix)*0.8):,]\n",
    "\n",
    "# training_feature = feature_matrix[:int(len(final_matrix)*0.9),]\n",
    "# training_groundtruth = label_matrix[:int(len(final_matrix)*0.9),]\n",
    "# test_feature = feature_matrix[int(len(final_matrix)*0.9):,]\n",
    "# test_groundtruth = label_matrix[int(len(final_matrix)*0.9):,]\n",
    "\n",
    "#print(len(test_groundtruth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Guessing: 0.49132676980778245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PROGRAMS\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Classification Acc: 0.7191748710736052\n",
      "[[770 271]\n",
      " [328 764]]\n",
      "SVM Classification Acc: 0.726207219878106\n",
      "[[775 266]\n",
      " [318 774]]\n",
      "NB Classification Acc: 0.7744960150023441\n",
      "[[842 199]\n",
      " [282 810]]\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "\n",
    "def LogisticRegressionClf(x_train, y_train):\n",
    "\tclf = LogisticRegression(penalty='l2')\n",
    "\tclf.fit(x_train, y_train)\n",
    "\treturn clf\n",
    "\n",
    "def SVMClf(x_train, y_train):\n",
    "\tclf = SVC(kernel='linear')\n",
    "\tclf.fit(x_train, y_train)\n",
    "\treturn clf \n",
    "\n",
    "def NaiveBayesClf(x_train, y_train):\n",
    "\tclf = MultinomialNB()\n",
    "\tclf.fit(x_train, y_train)\n",
    "\treturn clf\n",
    "\n",
    "\n",
    "#BASELINE: pure guessing\n",
    "rand_state = 180\n",
    "random.seed(rand_state)\n",
    "success_count = 0\n",
    "for result in test_groundtruth:\n",
    "\trandom_predict = random.randint(0,1)\n",
    "\tif(random_predict == result):\n",
    "\t\tsuccess_count += 1\n",
    "        \n",
    "\n",
    "print(\"Random Guessing: {}\".format((success_count * 1.0) / len(test_groundtruth)))\n",
    "\n",
    "model1 = LogisticRegressionClf(training_feature,training_groundtruth)\n",
    "print(\"LR Classification Acc:\", model1.score(test_feature, test_groundtruth))\n",
    "label_pred = model1.predict(test_feature)\n",
    "print(confusion_matrix(test_groundtruth, label_pred))\n",
    "\n",
    "model2 = SVMClf(training_feature,training_groundtruth)\n",
    "print(\"SVM Classification Acc:\", model2.score(test_feature, test_groundtruth))\n",
    "label_pred = model2.predict(test_feature)\n",
    "print(confusion_matrix(test_groundtruth, label_pred))\n",
    "\n",
    "model3 = NaiveBayesClf(training_feature,training_groundtruth)\n",
    "print(\"NB Classification Acc:\", model3.score(test_feature, test_groundtruth))\n",
    "label_pred = model3.predict(test_feature)\n",
    "print(confusion_matrix(test_groundtruth, label_pred))\n",
    "\n",
    "### This part is for cross-validation\n",
    "### I implemented this for practice (i know there's packages around)\n",
    "# def evaluation(prediction: np.ndarray, groundtruth: np.ndarray):\n",
    "#     # sanity check\n",
    "#     if len(prediction) != len(groundtruth):\n",
    "#         raise TypeError\n",
    "    \n",
    "#     tn,fp,fn,tp = 0,0,0,0 #true negative, false positive, false negative, true positive\n",
    "    \n",
    "#     for i in range(len(prediction)):\n",
    "#         if prediction[i] == 0 and groundtruth[i] == 0:\n",
    "#             tn += 1\n",
    "#         if prediction[i] == 1 and groundtruth[i] == 0:\n",
    "#             fp += 1\n",
    "#         if prediction[i] == 0 and groundtruth[i] == 1:\n",
    "#             fn += 1\n",
    "#         if prediction[i] == 1 and groundtruth[i] == 1:\n",
    "#             tp += 1\n",
    "#     return tn,fp,fn,tp\n",
    "\n",
    "\n",
    "# ################ This is the function to call for \"Accuracy\"############\n",
    "# def accuracy(prediction: np.ndarray, groundtruth: np.ndarray):\n",
    "#     tn,fp,fn,tp = evaluation(prediction,groundtruth)\n",
    "#     return 1.0*(tp+tn)/(tp+tn+fp+fn)\n",
    "\n",
    "\n",
    "# def merge_chunks(data_split,indices):\n",
    "#     indices = list(indices).sort()\n",
    "#     if len([indices]) < 2:\n",
    "#         return data_split[0]\n",
    "#     data_merged = data_split[indices[0]]\n",
    "#     indices.remove(indices[0]) #remove the first element so that it does not get re-merged\n",
    "#     for i in indices:\n",
    "#         data_merged = np.concatenate(data_merged,data_split[i],axis=0)\n",
    "        \n",
    "#     return data_merged\n",
    "        \n",
    "\n",
    "# def cross_validation(model,x: np.ndarray,y: np.ndarray, k: int):\n",
    "    \n",
    "#     data = np.zeros((len(x),len(x[0])+1))\n",
    "#     #combine and save to \"data\"\n",
    "#     for i in range(len(x)):\n",
    "#         data[i] = np.append(x[i],[y[i]])\n",
    "#     # print(data)\n",
    "#     np.random.shuffle(data)\n",
    "#     data_split = np.array_split(data,k)\n",
    "#     indices = set(range(k)) # a set containing 0 to k-1\n",
    "#     acc_list = [] # the list containing all the output accuracies by k folds\n",
    "#     for fold in range(k):\n",
    "#         # merge the numpy arrays except for the validation set for training\n",
    "#         other_indices = indices - set([fold])\n",
    "#         training_set = merge_chunks(data_split,other_indices)\n",
    "#         test_set = data_split[fold]\n",
    "#         x_train = training_set[:,:-1]\n",
    "#         y_train = training_set[:,-1]\n",
    "#         x_test = test_set[:,:-1]\n",
    "#         y_test = test_set[:,-1]\n",
    "        \n",
    "#         model.fit(x_train,y_train)\n",
    "#         y_prediction = model.predict(x_test)\n",
    "        \n",
    "#         acc_list.append(accuracy(y_prediction,y_test))\n",
    "#     return sum(acc_list) / len(acc_list)\n",
    "\n",
    "\n",
    "# The best result\n",
    "# Random Guessing: 0.49132676980778245\n",
    "# D:\\PROGRAMS\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
    "#   FutureWarning)\n",
    "# LR Classification Acc: 0.7191748710736052\n",
    "# [[770 271]\n",
    "#  [328 764]]\n",
    "# SVM Classification Acc: 0.726207219878106\n",
    "# [[775 266]\n",
    "#  [318 774]]\n",
    "# NB Classification Acc: 0.7744960150023441\n",
    "# [[842 199]\n",
    "#  [282 810]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
